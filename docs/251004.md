###251004 张艺豪

1. 历史天 + 当天 每个页面的总体访问量
2. 历史天 + 当天 共计搜索词TOP10(每天的词云)
3. 历史天 + 当天 登陆区域的全国热力情况(每个地区的访问值)
4. 历史天 + 当天 路径分析
5. 历史天 + 当天 用户设备的统计(ios & anzhuo (子类品牌(版本))) 下钻 (饼图 & 玫瑰图)
   package com.stream.realtime.lululemon.flinkapi.todoris;

import com.stream.core.ConfigUtils;
import com.stream.core.EnvironmentSettingUtils;
import com.stream.core.KafkaUtils;
import com.stream.core.WaterMarkUtils;
import lombok.Data;
import lombok.SneakyThrows;
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.java.functions.KeySelector;
import org.apache.flink.connector.jdbc.JdbcConnectionOptions;
import org.apache.flink.connector.jdbc.JdbcExecutionOptions;
import org.apache.flink.connector.jdbc.JdbcSink;
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.windowing.ProcessAllWindowFunction;
import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction;
import org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.api.windowing.windows.TimeWindow;
import org.apache.flink.util.Collector;
import com.alibaba.fastjson2.JSON;

import java.sql.Timestamp;
import java.time.Instant;
import java.time.LocalDateTime;
import java.time.ZoneId;
import java.time.format.DateTimeFormatter;
import java.util.*;

/**
* 用户路径分析任务 - 使用JDBC Sink写入Doris（借鉴页面访问量代码）
  */
  public class UserPathAnalysisTasktodoris {

  // Kafka 配置
  private static final String KAFKA_BOOTSTRAP_SERVERS = ConfigUtils.getString("kafka.bootstrap.servers");
  private static final String KAFKA_LOG_TOPIC = "realtime_v3_logs";

  // Doris 配置 - 使用与页面访问量相同的配置
  private static final String DORIS_URL = "jdbc:mysql://192.168.200.30:9030/flinkapi_lululemon";
  private static final String DORIS_USER = "root";
  private static final String DORIS_PASSWORD = "";

  // 时间格式化器
  private static final DateTimeFormatter DATE_FORMATTER = DateTimeFormatter.ofPattern("yyyy-MM-dd");
  private static final DateTimeFormatter TIME_FORMATTER = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss");

  // 用于统计Kafka累计条数的全局变量
  private static long totalKafkaRecords = 0L;

  /**
    * Kafka日志数据实体类
      */
      @Data
      public static class LogData {
      private String log_id;
      private Device device;
      private Gis gis;
      private Network network;
      private String opa;
      private String log_type;
      private Long ts;
      private String product_id;
      private String order_id;
      private String user_id;
      private java.util.List<String> keywords;
      }

  @Data
  public static class Device {
  private String brand;
  private String plat;
  private String platv;
  private String softv;
  private String uname;
  private String userkey;
  private String device;
  }

  @Data
  public static class Gis {
  private String ip;
  }

  @Data
  public static class Network {
  private String net;
  }

  /**
    * 用户行为事件实体类
      */
      @Data
      public static class UserEvent {
      private String userId;
      private String eventType;
      private Long timestamp;
      private String date;
      private String logId;
      private String productId;
      private String orderId;
      private String brand;
      private String ip;

      public UserEvent() {}

      public UserEvent(String userId, String eventType, Long timestamp, String date,
      String logId, String productId, String orderId, String brand, String ip) {
      this.userId = userId;
      this.eventType = eventType;
      this.timestamp = timestamp;
      this.date = date;
      this.logId = logId;
      this.productId = productId;
      this.orderId = orderId;
      this.brand = brand;
      this.ip = ip;
      }
      }

  /**
    * 用户路径记录实体类 - 对应Doris表结构
      */
      @Data
      public static class UserPathRecord {
      private String userId;           // 用户ID
      private String eventDate;        // 事件日期
      private String windowStartTime;  // 窗口开始时间
      private String pathSequence;     // 路径序列
      private Integer pathLength;      // 路径长度
      private String startEvent;       // 开始事件
      private String endEvent;         // 结束事件
      private String windowEndTime;    // 窗口结束时间
      private Timestamp processTime;   // 处理时间

      public UserPathRecord() {}

      public UserPathRecord(String userId, String eventDate, String windowStartTime,
      String pathSequence, Integer pathLength,
      String startEvent, String endEvent, String windowEndTime) {
      this.userId = userId;
      this.eventDate = eventDate;
      this.windowStartTime = windowStartTime;
      this.pathSequence = pathSequence;
      this.pathLength = pathLength;
      this.startEvent = startEvent;
      this.endEvent = endEvent;
      this.windowEndTime = windowEndTime;
      this.processTime = new Timestamp(System.currentTimeMillis());
      }
      }

  /**
    * 时间戳标准化和日期格式化方法
      */
      private static String normalizeAndFormatDate(Long timestamp) {
      if (timestamp == null) {
      return LocalDateTime.now().format(DATE_FORMATTER);
      }

      String timestampStr = String.valueOf(timestamp);
      long normalizedMillis;

      // 处理秒级时间戳和毫秒级时间戳
      if (timestampStr.length() == 10) {
      normalizedMillis = timestamp * 1000L;
      } else if (timestampStr.length() == 13) {
      normalizedMillis = timestamp;
      } else {
      normalizedMillis = System.currentTimeMillis();
      }

      try {
      LocalDateTime dateTime = LocalDateTime.ofInstant(
      Instant.ofEpochMilli(normalizedMillis), ZoneId.systemDefault());
      return dateTime.format(DATE_FORMATTER);
      } catch (Exception e) {
      return LocalDateTime.now().format(DATE_FORMATTER);
      }
      }

  /**
    * 时间戳格式化方法
      */
      private static String formatTimestamp(Long timestamp) {
      if (timestamp == null) {
      return LocalDateTime.now().format(TIME_FORMATTER);
      }

      String timestampStr = String.valueOf(timestamp);
      long normalizedMillis;

      if (timestampStr.length() == 10) {
      normalizedMillis = timestamp * 1000L;
      } else if (timestampStr.length() == 13) {
      normalizedMillis = timestamp;
      } else {
      normalizedMillis = System.currentTimeMillis();
      }

      try {
      LocalDateTime dateTime = LocalDateTime.ofInstant(
      Instant.ofEpochMilli(normalizedMillis), ZoneId.systemDefault());
      return dateTime.format(TIME_FORMATTER);
      } catch (Exception e) {
      return LocalDateTime.now().format(TIME_FORMATTER);
      }
      }

  @SneakyThrows
  public static void main(String[] args) {
  StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
  EnvironmentSettingUtils.defaultParameter(env);

       System.out.println("=== 启动用户路径分析作业 - 使用JDBC Sink写入Doris ===");

       // 1. 读取Kafka数据源
       DataStreamSource<String> originKafkaLogDs = env.fromSource(
               KafkaUtils.buildKafkaSource(KAFKA_BOOTSTRAP_SERVERS, KAFKA_LOG_TOPIC,
                       "user-path-analysis-group", OffsetsInitializer.earliest()),
               WaterMarkUtils.publicAssignWatermarkStrategy("ts", 5L),
               "kafka_log_source"
       );

       // 2. 统计Kafka累计条数（借鉴页面访问量的统计方式）
       originKafkaLogDs
               .map(new MapFunction<String, String>() {
                   @Override
                   public String map(String value) throws Exception {
                       synchronized (UserPathAnalysisTasktodoris.class) {
                           totalKafkaRecords++;
                       }
                       return value;
                   }
               })
               .keyBy(value -> "total_count")
               .process(new org.apache.flink.streaming.api.functions.KeyedProcessFunction<String, String, String>() {
                   private org.apache.flink.api.common.state.ValueState<Boolean> timerRegistered;
                   private org.apache.flink.api.common.state.ValueState<Long> lastReportCount;

                   @Override
                   public void open(org.apache.flink.configuration.Configuration parameters) {
                       timerRegistered = getRuntimeContext().getState(
                               new org.apache.flink.api.common.state.ValueStateDescriptor<>("timerRegistered", Boolean.class));
                       lastReportCount = getRuntimeContext().getState(
                               new org.apache.flink.api.common.state.ValueStateDescriptor<>("lastReportCount", Long.class));
                   }

                   @Override
                   public void processElement(String value, Context ctx, Collector<String> out) throws Exception {
                       if (timerRegistered.value() == null || !timerRegistered.value()) {
                           ctx.timerService().registerProcessingTimeTimer(ctx.timerService().currentProcessingTime() + 10000);
                           timerRegistered.update(true);
                       }
                   }

                   @Override
                   public void onTimer(long timestamp, OnTimerContext ctx, Collector<String> out) throws Exception {
                       long currentTotal = totalKafkaRecords;
                       Long lastCount = lastReportCount.value();

                       if (lastCount == null || currentTotal > lastCount) {
                           out.collect(String.format("累计读取Kafka数据: %d 条", currentTotal));
                           lastReportCount.update(currentTotal);
                       }

                       ctx.timerService().registerProcessingTimeTimer(ctx.timerService().currentProcessingTime() + 10000);
                   }
               })
               .print("Kafka数据统计");

       // 3. JSON解析
       SingleOutputStreamOperator<LogData> parsedStream = originKafkaLogDs
               .flatMap(new FlatMapFunction<String, LogData>() {
                   @Override
                   public void flatMap(String value, Collector<LogData> out) {
                       try {
                           LogData logData = JSON.parseObject(value, LogData.class);
                           out.collect(logData);
                       } catch (Exception e) {
                           System.err.println("JSON解析失败: " + e.getMessage());
                       }
                   }
               })
               .name("json-parser");

       // 4. 提取用户行为事件
       SingleOutputStreamOperator<UserEvent> userEventStream = parsedStream
               .flatMap(new FlatMapFunction<LogData, UserEvent>() {
                   @Override
                   public void flatMap(LogData logData, Collector<UserEvent> out) {
                       if (logData.getUser_id() != null && logData.getTs() != null && logData.getLog_type() != null) {
                           String userId = logData.getUser_id();
                           String eventType = normalizeEventType(logData.getLog_type());
                           Long timestamp = logData.getTs();
                           String date = normalizeAndFormatDate(timestamp);
                           String logId = logData.getLog_id();
                           String productId = logData.getProduct_id();
                           String orderId = logData.getOrder_id();
                           String brand = logData.getDevice() != null ? logData.getDevice().getBrand() : "unknown";
                           String ip = logData.getGis() != null ? logData.getGis().getIp() : "unknown";

                           UserEvent userEvent = new UserEvent(userId, eventType, timestamp, date,
                                   logId, productId, orderId, brand, ip);
                           out.collect(userEvent);
                       }
                   }

                   private String normalizeEventType(String logType) {
                       if (logType == null) {
                           return "unknown";
                       }
                       switch (logType.toLowerCase()) {
                           case "login":
                           case "home":
                           case "search":
                           case "product_list":
                           case "product_detail":
                           case "payment":
                               return logType.toLowerCase();
                           default:
                               return "other";
                       }
                   }
               })
               .name("user-event-extractor");

       // 5. 按用户分组，构建用户路径
       SingleOutputStreamOperator<UserPathRecord> userPathStream = userEventStream
               .keyBy(new KeySelector<UserEvent, String>() {
                   @Override
                   public String getKey(UserEvent event) throws Exception {
                       return event.getUserId() + "|" + event.getDate();
                   }
               })
               .window(TumblingProcessingTimeWindows.of(Time.minutes(5)))
               .process(new ProcessWindowFunction<UserEvent, UserPathRecord, String, TimeWindow>() {
                   @Override
                   public void process(String key, Context context, Iterable<UserEvent> elements,
                                       Collector<UserPathRecord> out) throws Exception {

                       List<UserEvent> events = new ArrayList<>();
                       for (UserEvent event : elements) {
                           events.add(event);
                       }

                       // 只有至少2个事件才构建路径
                       if (events.size() >= 2) {
                           // 按时间戳排序
                           events.sort(Comparator.comparing(UserEvent::getTimestamp));

                           // 构建路径序列 - 去除连续重复事件
                           StringBuilder pathBuilder = new StringBuilder();
                           List<String> uniqueEvents = new ArrayList<>();
                           String lastEvent = null;

                           for (UserEvent event : events) {
                               if (lastEvent == null || !event.getEventType().equals(lastEvent)) {
                                   pathBuilder.append(event.getEventType()).append("->");
                                   uniqueEvents.add(event.getEventType());
                                   lastEvent = event.getEventType();
                               }
                           }

                           // 移除最后的"->"
                           String path = pathBuilder.length() > 2 ?
                                   pathBuilder.substring(0, pathBuilder.length() - 2) : "";

                           String[] keyParts = key.split("\\|");
                           String userId = keyParts[0];
                           String date = keyParts.length > 1 ? keyParts[1] : "unknown";

                           String startEvent = uniqueEvents.isEmpty() ? "unknown" : uniqueEvents.get(0);
                           String endEvent = uniqueEvents.isEmpty() ? "unknown" :
                                   uniqueEvents.get(uniqueEvents.size() - 1);

                           // 创建用户路径记录
                           UserPathRecord userPath = new UserPathRecord(
                                   userId,
                                   date,
                                   LocalDateTime.ofInstant(Instant.ofEpochMilli(context.window().getStart()),
                                           ZoneId.systemDefault()).format(TIME_FORMATTER),
                                   path,
                                   uniqueEvents.size(),
                                   startEvent,
                                   endEvent,
                                   LocalDateTime.ofInstant(Instant.ofEpochMilli(context.window().getEnd()),
                                           ZoneId.systemDefault()).format(TIME_FORMATTER)
                           );
                           out.collect(userPath);
                       }
                   }
               })
               .name("user-path-builder");

       // 6. 写入用户路径到Doris（借鉴页面访问量的JDBC Sink方式）
       userPathStream.addSink(JdbcSink.sink(
               "INSERT INTO user_path_analysis " +
                       "(user_id, event_date, window_start_time, path_sequence, path_length, " +
                       "start_event, end_event, window_end_time, process_time) " +
                       "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)",
               (ps, record) -> {
                   ps.setString(1, record.getUserId());
                   ps.setString(2, record.getEventDate());
                   ps.setString(3, record.getWindowStartTime());
                   ps.setString(4, record.getPathSequence());
                   ps.setInt(5, record.getPathLength());
                   ps.setString(6, record.getStartEvent());
                   ps.setString(7, record.getEndEvent());
                   ps.setString(8, record.getWindowEndTime());
                   ps.setTimestamp(9, record.getProcessTime());
               },
               JdbcExecutionOptions.builder()
                       .withBatchSize(100)
                       .withBatchIntervalMs(2000)
                       .withMaxRetries(3)
                       .build(),
               new JdbcConnectionOptions.JdbcConnectionOptionsBuilder()
                       .withUrl(DORIS_URL)
                       .withDriverName("com.mysql.cj.jdbc.Driver")
                       .withUsername(DORIS_USER)
                       .withPassword(DORIS_PASSWORD)
                       .build()
       )).name("user_path_to_doris");

       // 7. 控制台输出用于监控
       userPathStream
               .map(new MapFunction<UserPathRecord, String>() {
                   @Override
                   public String map(UserPathRecord userPath) throws Exception {
                       return String.format("用户路径: userId=%s, date=%s, path=%s, length=%d",
                               userPath.getUserId(), userPath.getEventDate(),
                               userPath.getPathSequence(), userPath.getPathLength());
                   }
               })
               .print("user-path-output");

       // 8. 窗口统计信息输出
       userPathStream
               .map(new MapFunction<UserPathRecord, Long>() {
                   @Override
                   public Long map(UserPathRecord value) throws Exception {
                       return 1L;
                   }
               })
               .windowAll(TumblingProcessingTimeWindows.of(Time.minutes(5)))
               .process(new ProcessAllWindowFunction<Long, String, TimeWindow>() {
                   @Override
                   public void process(Context context, Iterable<Long> elements, Collector<String> out) throws Exception {
                       long windowPathCount = 0L;
                       for (Long element : elements) {
                           windowPathCount += element;
                       }

                       long currentTotalKafkaRecords;
                       synchronized (UserPathAnalysisTasktodoris.class) {
                           currentTotalKafkaRecords = totalKafkaRecords;
                       }

                       String stats = String.format(
                               "=== 窗口统计 ===\n" +
                                       "窗口结束时间: %s\n" +
                                       "本窗口分析路径数: %d\n" +
                                       "Kafka累计读取条数: %d\n" +
                                       "======================",
                               LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss")),
                               windowPathCount,
                               currentTotalKafkaRecords
                       );
                       out.collect(stats);
                   }
               })
               .print("window-statistics");

       env.execute("用户路径分析 - JDBC写入Doris");
  }
  }






package com.stream.realtime.lululemon.flinkapi.todoris;

import com.stream.core.ConfigUtils;
import com.stream.core.EnvironmentSettingUtils;
import com.stream.core.KafkaUtils;
import com.stream.core.WaterMarkUtils;
import lombok.Data;
import lombok.SneakyThrows;
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.state.MapState;
import org.apache.flink.api.common.state.MapStateDescriptor;
import org.apache.flink.api.common.state.ValueState;
import org.apache.flink.api.common.state.ValueStateDescriptor;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.connector.jdbc.JdbcConnectionOptions;
import org.apache.flink.connector.jdbc.JdbcExecutionOptions;
import org.apache.flink.connector.jdbc.JdbcSink;
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.KeyedProcessFunction;
import org.apache.flink.streaming.api.functions.windowing.ProcessAllWindowFunction;
import org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.api.windowing.windows.TimeWindow;
import org.apache.flink.util.Collector;
import com.alibaba.fastjson2.JSON;
import com.alibaba.fastjson2.JSONObject;

import java.sql.Timestamp;
import java.time.Instant;
import java.time.LocalDateTime;
import java.time.ZoneId;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.stream.Collectors;

/**
* 每日搜索关键词TOP10统计 - 使用JDBC Sink写入Doris（修复排名问题）
  */
  public class SearchTermTotalAggregatorApitodoris1 {

  // Kafka 配置
  private static final String KAFKA_BOOTSTRAP_SERVERS = ConfigUtils.getString("kafka.bootstrap.servers");
  private static final String KAFKA_LOG_TOPIC = "realtime_v3_logs";

  // Doris 配置
  private static final String DORIS_URL = "jdbc:mysql://192.168.200.30:9030/flinkapi_lululemon";
  private static final String DORIS_USER = "root";
  private static final String DORIS_PASSWORD = "";

  // 时间格式化器
  private static final DateTimeFormatter DATE_FORMATTER = DateTimeFormatter.ofPattern("yyyy-MM-dd");
  private static final DateTimeFormatter TIME_FORMATTER = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss");

  // 用于统计Kafka累计条数的全局变量
  private static long totalKafkaRecords = 0L;

  /**
    * 搜索日志数据实体类
      */
      @Data
      public static class SearchLog {
      private String logType;
      private Long ts;
      private List<String> keywords;
      private String userId;
      private String productId;
      private String orderId;
      private String rawData;
      }

  /**
    * 关键词事件实体类
      */
      @Data
      public static class KeywordEvent {
      private String keyword;
      private String date;
      private Long timestamp;

      public KeywordEvent() {}

      public KeywordEvent(String keyword, String date, Long timestamp) {
      this.keyword = keyword;
      this.date = date;
      this.timestamp = timestamp;
      }
      }

  /**
    * 关键词统计实体类
      */
      @Data
      public static class KeywordStats {
      private String keyword;
      private String date;
      private Long searchCount;
      private Long processTime;

      public KeywordStats() {}

      public KeywordStats(String keyword, String date, Long searchCount) {
      this.keyword = keyword;
      this.date = date;
      this.searchCount = searchCount;
      this.processTime = System.currentTimeMillis();
      }
      }

  /**
    * 关键词排名实体类
      */
      @Data
      public static class KeywordRank {
      private String keyword;
      private Long totalCount;
      private Integer rank;

      @Override
      public String toString() {
      return String.format("#%d %s (搜索次数: %d)", rank, keyword, totalCount);
      }
      }

  /**
    * 每日TOP10结果实体类
      */
      @Data
      public static class DailyTop10Result {
      private String date;
      private List<KeywordRank> top10Keywords;
      private Integer totalUniqueKeywords;
      private Integer totalProcessedRecords;
      private Long processTime;

      @Override
      public String toString() {
      StringBuilder sb = new StringBuilder();
      sb.append("\n").append(generateSeparator(50));
      sb.append("\n           每日搜索词TOP10词云");
      sb.append("\n").append(generateSeparator(50));
      sb.append("\n日期: ").append(date);
      sb.append("\n总关键词数: ").append(totalUniqueKeywords);
      sb.append("\n处理记录数: ").append(totalProcessedRecords);
      sb.append("\n").append(generateSeparator(50));
      sb.append("\nTOP10搜索词:\n");

           for (KeywordRank rank : top10Keywords) {
               sb.append("   ").append(rank.toString()).append("\n");
           }

           sb.append(generateSeparator(50));
           sb.append("\n更新时间: ").append(
                   LocalDateTime.ofInstant(Instant.ofEpochMilli(processTime), ZoneId.systemDefault())
                           .format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss")));
           sb.append("\n").append(generateSeparator(50));
           return sb.toString();
      }

      private String generateSeparator(int length) {
      StringBuilder separator = new StringBuilder();
      for (int i = 0; i < length; i++) {
      separator.append("=");
      }
      return separator.toString();
      }
      }

  /**
    * TOP10记录实体类 - 对应Doris表结构
      */
      @Data
      public static class Top10Record {
      private String statDate;              // 统计日期
      private String keyword;               // 搜索关键词
      private Long totalCount;              // 总搜索次数
      private Integer dailyRank;            // 当日排名
      private Integer totalUniqueKeywords;  // 总关键词数
      private Integer totalProcessedRecords; // 处理记录数
      private Timestamp processTime;        // 处理时间
      private Timestamp updateTime;         // 更新时间

      public Top10Record() {}

      public Top10Record(String statDate, String keyword, Long totalCount, Integer dailyRank,
      Integer totalUniqueKeywords, Integer totalProcessedRecords) {
      this.statDate = statDate;
      this.keyword = keyword;
      this.totalCount = totalCount;
      this.dailyRank = dailyRank;
      this.totalUniqueKeywords = totalUniqueKeywords;
      this.totalProcessedRecords = totalProcessedRecords;
      this.processTime = new Timestamp(System.currentTimeMillis());
      this.updateTime = new Timestamp(System.currentTimeMillis());
      }
      }

  /**
    * 时间戳标准化和日期格式化方法
      */
      private static String normalizeAndFormatDate(Long timestamp) {
      if (timestamp == null) {
      return LocalDateTime.now().format(DATE_FORMATTER);
      }

      String timestampStr = String.valueOf(timestamp);
      long normalizedMillis;

      // 处理秒级时间戳和毫秒级时间戳
      if (timestampStr.length() == 10) {
      normalizedMillis = timestamp * 1000L;
      } else if (timestampStr.length() == 13) {
      normalizedMillis = timestamp;
      } else {
      normalizedMillis = System.currentTimeMillis();
      }

      try {
      LocalDateTime dateTime = LocalDateTime.ofInstant(
      Instant.ofEpochMilli(normalizedMillis), ZoneId.systemDefault());
      return dateTime.format(DATE_FORMATTER);
      } catch (Exception e) {
      return LocalDateTime.now().format(DATE_FORMATTER);
      }
      }

  @SneakyThrows
  public static void main(String[] args) {
  StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
  EnvironmentSettingUtils.defaultParameter(env);

       System.out.println("=== 启动每日搜索关键词TOP10统计作业 - 使用JDBC Sink写入Doris（修复排名问题） ===");

       // 1. 读取Kafka数据源
       DataStreamSource<String> originKafkaLogDs = env.fromSource(
               KafkaUtils.buildKafkaSource(KAFKA_BOOTSTRAP_SERVERS, KAFKA_LOG_TOPIC,
                       "search-keyword-top10-group", OffsetsInitializer.earliest()),
               WaterMarkUtils.publicAssignWatermarkStrategy("ts", 5L),
               "kafka_search_log_source"
       );

       // 2. 统计Kafka累计条数
       originKafkaLogDs
               .map(new MapFunction<String, String>() {
                   @Override
                   public String map(String value) throws Exception {
                       synchronized (SearchTermTotalAggregatorApitodoris1.class) {
                           totalKafkaRecords++;
                       }
                       return value;
                   }
               })
               .keyBy(value -> "total_count")
               .process(new org.apache.flink.streaming.api.functions.KeyedProcessFunction<String, String, String>() {
                   private org.apache.flink.api.common.state.ValueState<Boolean> timerRegistered;
                   private org.apache.flink.api.common.state.ValueState<Long> lastReportCount;

                   @Override
                   public void open(org.apache.flink.configuration.Configuration parameters) {
                       timerRegistered = getRuntimeContext().getState(
                               new org.apache.flink.api.common.state.ValueStateDescriptor<>("timerRegistered", Boolean.class));
                       lastReportCount = getRuntimeContext().getState(
                               new org.apache.flink.api.common.state.ValueStateDescriptor<>("lastReportCount", Long.class));
                   }

                   @Override
                   public void processElement(String value, Context ctx, Collector<String> out) throws Exception {
                       if (timerRegistered.value() == null || !timerRegistered.value()) {
                           ctx.timerService().registerProcessingTimeTimer(ctx.timerService().currentProcessingTime() + 10000);
                           timerRegistered.update(true);
                       }
                   }

                   @Override
                   public void onTimer(long timestamp, OnTimerContext ctx, Collector<String> out) throws Exception {
                       long currentTotal = totalKafkaRecords;
                       Long lastCount = lastReportCount.value();

                       if (lastCount == null || currentTotal > lastCount) {
                           out.collect(String.format("累计读取Kafka数据: %d 条", currentTotal));
                           lastReportCount.update(currentTotal);
                       }

                       ctx.timerService().registerProcessingTimeTimer(ctx.timerService().currentProcessingTime() + 10000);
                   }
               })
               .print("Kafka数据统计");

       // 3. JSON解析
       SingleOutputStreamOperator<SearchLog> parsedStream = originKafkaLogDs
               .flatMap(new FlatMapFunction<String, SearchLog>() {
                   @Override
                   public void flatMap(String value, Collector<SearchLog> out) {
                       try {
                           JSONObject jsonObject = JSON.parseObject(value);
                           SearchLog log = new SearchLog();
                           log.setLogType(jsonObject.getString("log_type"));

                           // 处理时间戳，兼容10位和13位
                           Object tsObj = jsonObject.get("ts");
                           if (tsObj != null) {
                               if (tsObj instanceof Long) {
                                   log.setTs((Long) tsObj);
                               } else if (tsObj instanceof Integer) {
                                   log.setTs(((Integer) tsObj).longValue());
                               } else {
                                   try {
                                       String tsStr = tsObj.toString();
                                       long ts = Long.parseLong(tsStr);
                                       log.setTs(ts);
                                   } catch (NumberFormatException e) {
                                       System.err.println("Invalid timestamp format: " + tsObj);
                                       log.setTs(System.currentTimeMillis() / 1000);
                                   }
                               }
                           } else {
                               log.setTs(System.currentTimeMillis() / 1000);
                           }

                           log.setRawData(value);

                           // 解析keywords数组
                           if (jsonObject.containsKey("keywords")) {
                               try {
                                   List<String> keywords = jsonObject.getJSONArray("keywords")
                                           .toList(String.class);
                                   log.setKeywords(keywords);
                               } catch (Exception e) {
                                   System.err.println("Keywords parse error: " + e.getMessage());
                                   log.setKeywords(null);
                               }
                           } else {
                               log.setKeywords(null);
                           }

                           log.setUserId(jsonObject.getString("user_id"));
                           log.setProductId(jsonObject.getString("product_id"));
                           log.setOrderId(jsonObject.getString("order_id"));
                           out.collect(log);
                       } catch (Exception e) {
                           System.err.println("JSON解析失败: " + e.getMessage());
                       }
                   }
               })
               .name("json-parser");

       // 4. 提取搜索关键词
       SingleOutputStreamOperator<KeywordEvent> keywordStream = parsedStream
               .flatMap(new FlatMapFunction<SearchLog, KeywordEvent>() {
                   @Override
                   public void flatMap(SearchLog searchLog, Collector<KeywordEvent> out) {
                       if (searchLog.getKeywords() != null && !searchLog.getKeywords().isEmpty()
                               && "search".equals(searchLog.getLogType())) {
                           long timestamp = searchLog.getTs();
                           String date = normalizeAndFormatDate(timestamp);

                           for (String keyword : searchLog.getKeywords()) {
                               if (isValidKeyword(keyword)) {
                                   KeywordEvent event = new KeywordEvent(keyword.trim(), date, timestamp);
                                   out.collect(event);
                               }
                           }
                       }
                   }

                   private boolean isValidKeyword(String keyword) {
                       return keyword != null && !keyword.trim().isEmpty() && keyword.length() <= 100;
                   }
               })
               .name("keyword-extractor");

       // 5. 按日期和关键词分组统计搜索次数
       SingleOutputStreamOperator<KeywordStats> keywordStatsStream = keywordStream
               .keyBy(event -> event.getDate() + "|" + event.getKeyword())
               .process(new KeyedProcessFunction<String, KeywordEvent, KeywordStats>() {

                   private transient ValueState<Long> countState;

                   @Override
                   public void open(Configuration parameters) throws Exception {
                       ValueStateDescriptor<Long> countDescriptor =
                               new ValueStateDescriptor<>("countState", Types.LONG);
                       countState = getRuntimeContext().getState(countDescriptor);
                   }

                   @Override
                   public void processElement(KeywordEvent event, Context ctx, Collector<KeywordStats> out) throws Exception {
                       Long currentCount = countState.value();
                       if (currentCount == null) {
                           currentCount = 0L;
                       }
                       currentCount += 1;
                       countState.update(currentCount);

                       KeywordStats stats = new KeywordStats(event.getKeyword(), event.getDate(), currentCount);
                       out.collect(stats);
                   }
               })
               .name("keyword-stats-processor");

       // 6. 按日期分组，在全局范围内计算TOP10排名
       SingleOutputStreamOperator<DailyTop10Result> top10Stream = keywordStatsStream
               .keyBy(KeywordStats::getDate)
               .process(new KeyedProcessFunction<String, KeywordStats, DailyTop10Result>() {

                   private transient MapState<String, Long> keywordCounts;
                   private transient ValueState<Integer> processCounterState;
                   private transient ValueState<Long> lastOutputTimeState;

                   @Override
                   public void open(Configuration parameters) throws Exception {
                       MapStateDescriptor<String, Long> countsDescriptor =
                               new MapStateDescriptor<>("keywordCounts", Types.STRING, Types.LONG);
                       keywordCounts = getRuntimeContext().getMapState(countsDescriptor);

                       ValueStateDescriptor<Integer> counterDescriptor =
                               new ValueStateDescriptor<>("processCounter", Types.INT);
                       processCounterState = getRuntimeContext().getState(counterDescriptor);

                       ValueStateDescriptor<Long> timeDescriptor =
                               new ValueStateDescriptor<>("lastOutputTime", Types.LONG);
                       lastOutputTimeState = getRuntimeContext().getState(timeDescriptor);
                   }

                   @Override
                   public void processElement(KeywordStats stats, Context ctx, Collector<DailyTop10Result> out) throws Exception {
                       // 更新关键词计数
                       keywordCounts.put(stats.getKeyword(), stats.getSearchCount());

                       // 更新计数器
                       Integer processCount = processCounterState.value();
                       if (processCount == null) {
                           processCount = 0;
                       }
                       processCount++;
                       processCounterState.update(processCount);

                       // 获取上次输出时间
                       Long lastOutputTime = lastOutputTimeState.value();
                       long currentTime = System.currentTimeMillis();

                       // 每处理30条记录或每30秒输出一次TOP10
                       if (processCount % 30 == 0 ||
                               lastOutputTime == null ||
                               currentTime - lastOutputTime > 30000) {

                           outputTop10Result(stats.getDate(), out, processCount);
                           lastOutputTimeState.update(currentTime);
                       }
                   }

                   private void outputTop10Result(String date, Collector<DailyTop10Result> out, int processCount) throws Exception {
                       // 获取该日期所有关键词并按计数排序
                       List<Map.Entry<String, Long>> allKeywords = new ArrayList<>();
                       for (Map.Entry<String, Long> entry : keywordCounts.entries()) {
                           allKeywords.add(entry);
                       }

                       // 按计数降序排序
                       allKeywords.sort((a, b) -> Long.compare(b.getValue(), a.getValue()));

                       // 取TOP10并设置正确的排名
                       List<KeywordRank> top10 = new ArrayList<>();
                       for (int i = 0; i < Math.min(10, allKeywords.size()); i++) {
                           Map.Entry<String, Long> entry = allKeywords.get(i);
                           KeywordRank rank = new KeywordRank();
                           rank.setKeyword(entry.getKey());
                           rank.setTotalCount(entry.getValue());
                           rank.setRank(i + 1);  // 正确的全局排名
                           top10.add(rank);
                       }

                       // 创建TOP10结果
                       DailyTop10Result result = new DailyTop10Result();
                       result.setDate(date);
                       result.setTop10Keywords(top10);
                       result.setTotalUniqueKeywords(allKeywords.size());
                       result.setTotalProcessedRecords(processCount);
                       result.setProcessTime(System.currentTimeMillis());

                       out.collect(result);

                       // 输出调试信息
                       System.out.println("日期 " + date + " 的TOP10排名计算完成:");
                       for (KeywordRank rank : top10) {
                           System.out.println("  排名 " + rank.getRank() + ": " + rank.getKeyword() + " (" + rank.getTotalCount() + "次)");
                       }
                   }
               })
               .name("top10-processor");

       // 7. 将TOP10结果转换为Doris记录格式
       SingleOutputStreamOperator<Top10Record> dorisRecordStream = top10Stream
               .flatMap(new FlatMapFunction<DailyTop10Result, Top10Record>() {
                   @Override
                   public void flatMap(DailyTop10Result top10Result, Collector<Top10Record> out) throws Exception {
                       if (top10Result.getTop10Keywords() != null && !top10Result.getTop10Keywords().isEmpty()) {
                           for (KeywordRank rank : top10Result.getTop10Keywords()) {
                               Top10Record record = new Top10Record(
                                       top10Result.getDate(),
                                       rank.getKeyword(),
                                       rank.getTotalCount(),
                                       rank.getRank(),  // 这里现在使用正确的全局排名
                                       top10Result.getTotalUniqueKeywords(),
                                       top10Result.getTotalProcessedRecords()
                               );
                               out.collect(record);
                           }

                           System.out.println("转换 " + top10Result.getTop10Keywords().size() +
                                   " 条TOP10记录，日期: " + top10Result.getDate());
                       }
                   }
               })
               .name("doris-record-converter");

       // 8. 写入TOP10结果到Doris
       dorisRecordStream.addSink(JdbcSink.sink(
               "INSERT INTO search_keyword_top10_daily " +
                       "(stat_date, keyword, total_count, daily_rank, total_unique_keywords, " +
                       "total_processed_records, process_time, update_time) " +
                       "VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
               (ps, record) -> {
                   ps.setString(1, record.getStatDate());
                   ps.setString(2, record.getKeyword());
                   ps.setLong(3, record.getTotalCount());
                   ps.setInt(4, record.getDailyRank());
                   ps.setInt(5, record.getTotalUniqueKeywords());
                   ps.setInt(6, record.getTotalProcessedRecords());
                   ps.setTimestamp(7, record.getProcessTime());
                   ps.setTimestamp(8, record.getUpdateTime());
               },
               JdbcExecutionOptions.builder()
                       .withBatchSize(100)
                       .withBatchIntervalMs(2000)
                       .withMaxRetries(3)
                       .build(),
               new JdbcConnectionOptions.JdbcConnectionOptionsBuilder()
                       .withUrl(DORIS_URL)
                       .withDriverName("com.mysql.cj.jdbc.Driver")
                       .withUsername(DORIS_USER)
                       .withPassword(DORIS_PASSWORD)
                       .build()
       )).name("top10_to_doris");

       // 9. 控制台输出用于监控
       top10Stream
               .map(new MapFunction<DailyTop10Result, String>() {
                   @Override
                   public String map(DailyTop10Result result) throws Exception {
                       return String.format("TOP10结果: date=%s, totalKeywords=%d, processedRecords=%d",
                               result.getDate(), result.getTotalUniqueKeywords(), result.getTotalProcessedRecords());
                   }
               })
               .print("top10-output");

       env.execute("每日搜索关键词TOP10统计 - JDBC写入Doris（修复排名问题）");
  }
  }